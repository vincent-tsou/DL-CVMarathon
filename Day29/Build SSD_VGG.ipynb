{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssd import build_ssd\n",
    "from layers.box_utils import *\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torchvision\n",
    "import pickle\n",
    "from layers import box_utils\n",
    "from layers import Detect\n",
    "from layers import functions\n",
    "from layers import modules\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt as sqrt\n",
    "from itertools import product as product\n",
    "\n",
    "from torch.autograd import Function\n",
    "from layers.box_utils import decode, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading weights into state dict...\nFinished!\n"
    }
   ],
   "source": [
    "## 詳細模型結構可以參考ssd.py\n",
    "ssd_net=build_ssd('train', size=300, num_classes=21)\n",
    "ssd_net.load_weights('./demo/ssd300_mAP_77.43_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 默認Config檔案在data/config.py內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'num_classes': 21,\n 'lr_steps': (80000, 100000, 120000),\n 'max_iter': 120000,\n 'feature_maps': [38, 19, 10, 5, 3, 1],\n 'min_dim': 300,\n 'steps': [8, 16, 32, 64, 100, 300],\n 'min_sizes': [30, 60, 111, 162, 213, 264],\n 'max_sizes': [60, 111, 162, 213, 264, 315],\n 'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n 'variance': [0.1, 0.2],\n 'clip': True,\n 'name': 'VOC'}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "ssd_net.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'num_classes': 21,\n",
    "    'lr_steps': (80000, 100000, 120000),\n",
    "    'max_iter': 120000,\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'min_dim': 300,\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': True,\n",
    "    'name': 'VOC',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'aspect_ratios' : 使用六張Feature Map，每一張上方有預設的anchor boxes，Boxes aspect ratio可以自己設定\n",
    "### 'feature_maps' : 使用feature map大小為[38x38, 19x19, 10x10, 5x5, 3x3, 1x1]\n",
    "### 'min_sizes'、'max_sizes'可藉由下方算式算出，由作者自行設計\n",
    "### 'steps' : Feature map回放回原本300*300的比例，如38要回放為300大概就是8倍\n",
    "### 'variance' : Training 的一個trick，加速收斂，詳見：https://github.com/rykov8/ssd_keras/issues/53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'min_sizes'、'max_sizes' 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "min_sizes:  [30.0, 60.0, 111.0, 162.0, 213.0, 264.0]\nmax_sizes:  [60.0, 111.0, 162.0, 213.0, 264.0, 315.0]\n"
    }
   ],
   "source": [
    "import math\n",
    "## source:https://blog.csdn.net/gbyy42299/article/details/81235891\n",
    "min_dim = 300   ## 维度\n",
    "# conv4_3 ==> 38 x 38\n",
    "# fc7 ==> 19 x 19\n",
    "# conv6_2 ==> 10 x 10\n",
    "# conv7_2 ==> 5 x 5\n",
    "# conv8_2 ==> 3 x 3\n",
    "# conv9_2 ==> 1 x 1\n",
    "mbox_source_layers = ['conv4_3', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2'] ## prior_box來源層，可以更改。很多改進都是基於此處的調整。\n",
    "# in percent %\n",
    "min_ratio = 20 ## 這裡即是論文中所說的Smin的= 0.2，Smax的= 0.9的初始值，經過下面的運算即可得到min_sizes，max_sizes。\n",
    "max_ratio = 90\n",
    "step = int(math.floor((max_ratio - min_ratio) / (len(mbox_source_layers) - 2)))## 取一個間距步長，即在下面用於循環給比取值時起一個間距作用。可以用一個具體的數值代替，這裡等於17。\n",
    "min_sizes = []  ## 經過以下運算得到min_sizes和max_sizes。\n",
    "max_sizes = []\n",
    "for ratio in range(min_ratio, max_ratio + 1, step):\n",
    "    ## 從min_ratio至max_ratio + 1每隔步驟= 17取一個值賦值給比。注意範圍函數的作用。\n",
    "    ## min_sizes.append（）函數即把括號內部每次得到的值依次給了min_sizes。\n",
    "    min_sizes.append(min_dim * ratio / 100.)\n",
    "    max_sizes.append(min_dim * (ratio + step) / 100.)\n",
    "min_sizes = [min_dim * 10 / 100.] + min_sizes\n",
    "max_sizes = [min_dim * 20 / 100.] + max_sizes\n",
    "\n",
    "## steps: 這一步要仔細理解，即計算卷積層產生的prior_box距離原圖的步長，先驗框中心點的坐標會乘以step，\n",
    "## 相當於從特徵映射位置映射回原圖位置，比如conv4_3輸出特徵圖大小為38 *38，而輸入的圖片為300* 300，\n",
    "## 所以38 *8約等於300，所以映射步長為8.這是針對300* 300的訓練圖片。\n",
    "steps = [8, 16, 32, 64, 100, 300]  \n",
    "aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    " \n",
    "print('min_sizes: ',min_sizes)\n",
    "print('max_sizes: ',max_sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default anchor boxes設計原理，看懂收穫很多\n",
    "##### 可以理解 SSD原文中 8732個anchors是怎麼來的\n",
    "##### 38×38×4+19×19×6+10×10×6+5×5×6+3×3×4+1×1×4=8732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorBox(object):\n",
    "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
    "    feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip']\n",
    "        self.version = cfg['name']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        '''依照Feature map大小找出所有的pixel 中心'''\n",
    "        '''下方這兩個loop會找出W個x軸pixel對上W個y軸pixel，假如現在是在38x38的feature map上，就會有38x38個值'''\n",
    "        '''ex. [0,1],[0,2]..[0,37] [1,1],[1,2]..[1,37]..........[37,37]'''\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k] ## 如self.steps==8，就是先將原圖size normalize(/300)後再乘上8\n",
    "                # unit center x,y\n",
    "                '''中心點'''\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: min_size\n",
    "                '''/self.image_size 就是在做normalization '''\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                '''小的正方形box'''\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: sqrt(s_k * s_(k+1))\n",
    "                '''大的正方形box'''\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    '''aspect ratio 2,3'''\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    '''aspect ratio 1/2,1/3'''\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PriorBox_Demo=PriorBox(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([8732, 4])\n"
    }
   ],
   "source": [
    "print(PriorBox_Demo.forward().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss 如何設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 use_gpu=True):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        '''有幾類'''                \n",
    "        self.num_classes = num_classes\n",
    "        '''判定為正樣本的threshold，一般設為0.5'''\n",
    "        self.threshold = overlap_thresh\n",
    "        '''background自己會有一類，不用Label，假如我們有20類一樣標註0-19，下方會自己空出一類給background'''\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        '''OHEM，找出分得最不好的樣品，也就是confidence score比較低的正負樣品'''\n",
    "        self.do_neg_mining = neg_mining\n",
    "        '''負樣品與正樣品的比例，通常是3:1'''\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = cfg['variance']\n",
    "     \n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "\n",
    "        '''prediction會output三個值'''\n",
    "        '''loc shape: bounding box 資訊，torch.size(batch_size,num_priors,4)'''\n",
    "        '''conf shape: 每一個bounding box 的信心程度，torch.size(batch_size,num_priors,num_classes)'''\n",
    "        '''priors shape: 預設的defaul box， torch.size(num_priors,4)'''\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        if(self.use_gpu):\n",
    "            loc_t=loc_t.type(torch.cuda.FloatTensor)\n",
    "            conf_t=conf_t.type(torch.cuda.LongTensor)\n",
    "\n",
    "\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            if(self.use_gpu):\n",
    "                truths=truths.type(torch.cuda.FloatTensor)\n",
    "                labels=labels.type(torch.cuda.FloatTensor)\n",
    "                defaults=defaults.type(torch.cuda.FloatTensor)\n",
    "            '''jaccard 計算每一個BBOX與ground truth的IOU'''\n",
    "            match(self.threshold, truths, defaults, self.variance, labels,\n",
    "                  loc_t, conf_t, idx)\n",
    "        if self.use_gpu:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "        '''用Variable包裝'''\n",
    "        \n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        \n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        '''smooth_l1_loss 計算bounding box regression'''\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "         \n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        loss_c[pos] = 0\n",
    "\n",
    "        '''排列confidence 的分數'''\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "\n",
    "        '''負樣品取出數量 == negpos_ratio*num_pos'''\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "  \n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        '''用cross_entropy做分類'''\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        #double轉成torch.float64\n",
    "        N = num_pos.data.sum().double()\n",
    "        loss_l = loss_l.double()\n",
    "        loss_c = loss_c.double()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 產生我們Loss function，注意這裡的class要包含背景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use_cuda=True\n",
    "criterion = MultiBoxLoss(21, 0.5, True, 0, False, 3, 0.5,False, Use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading weights into state dict...\nFinished!\n"
    }
   ],
   "source": [
    "ssd_net=build_ssd('train', size=300, num_classes=21)\n",
    "use_pretrained=True\n",
    "if use_pretrained:\n",
    "    ssd_net.load_weights('./demo/ssd300_mAP_77.43_v2.pth')\n",
    "net=ssd_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''要不要使用gpu'''\n",
    "Use_cuda=True\n",
    "\n",
    "'''tensor type會依照cpu或gpu有所不同'''\n",
    "if torch.cuda.is_available():\n",
    "    if Use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if not Use_cuda:\n",
    "        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n",
    "              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "'''使用GPU時可以開啟DataParallel，但當Input是不定大小時，要關掉'''\n",
    "if Use_cuda:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "'''使用GPU時模型要轉成cuda'''\n",
    "if Use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "batch_size_=1\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.00001/batch_size_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 這裡我們先示範輸入的 image,Label格式，真正在訓練時，準備成一樣格式即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''輸入影像格式，假設batch size 為 4'''\n",
    "image_in=torch.tensor(torch.rand(4,3,300,300),dtype=torch.float32)\n",
    "'''Label格式，沒有固定長度，看圖像中有幾個label就有幾個'''\n",
    "label_0=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n",
    "label_1=[[ 0.1804,  0.6076,  0.7701,  0.8485, 13.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 11.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 7.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 5.0000],]\n",
    "label_2=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 14.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n",
    "label_3=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
    "       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n",
    "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10-7\n",
    "iteration=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "load weights\nLoading weights into state dict...\nFinished!\n"
    }
   ],
   "source": [
    "if(os.path.exists('weights/Ｗeights.pth')):\n",
    "    print('load weights')\n",
    "    ssd_net.load_weights('weights/Ｗeights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sification Loss: 0.802267\n0 70 BBOX Regression Loss: 0.018262  Classification Loss: 0.701793\n0 80 BBOX Regression Loss: 0.016677  Classification Loss: 0.623100\n0 90 BBOX Regression Loss: 0.015357  Classification Loss: 0.559889\n0 100 BBOX Regression Loss: 0.014235  Classification Loss: 0.508040\n0 110 BBOX Regression Loss: 0.013267  Classification Loss: 0.464768\n0 120 BBOX Regression Loss: 0.012431  Classification Loss: 0.428126\n0 130 BBOX Regression Loss: 0.011702  Classification Loss: 0.396724\n0 140 BBOX Regression Loss: 0.011057  Classification Loss: 0.369534\n0 150 BBOX Regression Loss: 0.010480  Classification Loss: 0.345776\n0 160 BBOX Regression Loss: 0.009959  Classification Loss: 0.324846\n0 170 BBOX Regression Loss: 0.009485  Classification Loss: 0.306270\n0 180 BBOX Regression Loss: 0.009049  Classification Loss: 0.289675\n0 190 BBOX Regression Loss: 0.008647  Classification Loss: 0.274764\n0 200 BBOX Regression Loss: 0.008276  Classification Loss: 0.261294\n0 210 BBOX Regression Loss: 0.007932  Classification Loss: 0.249068\n0 220 BBOX Regression Loss: 0.007613  Classification Loss: 0.237922\n0 230 BBOX Regression Loss: 0.007315  Classification Loss: 0.227721\n0 240 BBOX Regression Loss: 0.007038  Classification Loss: 0.218349\n0 250 BBOX Regression Loss: 0.006778  Classification Loss: 0.209711\n0 260 BBOX Regression Loss: 0.006535  Classification Loss: 0.201724\n0 270 BBOX Regression Loss: 0.006308  Classification Loss: 0.194318\n0 280 BBOX Regression Loss: 0.006094  Classification Loss: 0.187431\n0 290 BBOX Regression Loss: 0.005895  Classification Loss: 0.181012\n0 300 BBOX Regression Loss: 0.005707  Classification Loss: 0.175015\n0 310 BBOX Regression Loss: 0.005530  Classification Loss: 0.169399\n0 320 BBOX Regression Loss: 0.005363  Classification Loss: 0.164131\n0 330 BBOX Regression Loss: 0.005205  Classification Loss: 0.159179\n0 340 BBOX Regression Loss: 0.005056  Classification Loss: 0.154515\n0 350 BBOX Regression Loss: 0.004914  Classification Loss: 0.150116\n0 360 BBOX Regression Loss: 0.004780  Classification Loss: 0.145960\n0 370 BBOX Regression Loss: 0.004653  Classification Loss: 0.142027\n0 380 BBOX Regression Loss: 0.004532  Classification Loss: 0.138299\n0 390 BBOX Regression Loss: 0.004418  Classification Loss: 0.134762\n0 400 BBOX Regression Loss: 0.004308  Classification Loss: 0.131401\n0 410 BBOX Regression Loss: 0.004204  Classification Loss: 0.128203\n0 420 BBOX Regression Loss: 0.004105  Classification Loss: 0.125157\n0 430 BBOX Regression Loss: 0.004010  Classification Loss: 0.122252\n0 440 BBOX Regression Loss: 0.003919  Classification Loss: 0.119478\n0 450 BBOX Regression Loss: 0.003832  Classification Loss: 0.116827\n0 460 BBOX Regression Loss: 0.003749  Classification Loss: 0.114292\n0 470 BBOX Regression Loss: 0.003670  Classification Loss: 0.111863\n0 480 BBOX Regression Loss: 0.003594  Classification Loss: 0.109536\n0 490 BBOX Regression Loss: 0.003521  Classification Loss: 0.107304\n0 500 BBOX Regression Loss: 0.003450  Classification Loss: 0.105160\n0 510 BBOX Regression Loss: 0.003383  Classification Loss: 0.103101\n0 520 BBOX Regression Loss: 0.003318  Classification Loss: 0.101121\n0 530 BBOX Regression Loss: 0.003255  Classification Loss: 0.099215\n0 540 BBOX Regression Loss: 0.003195  Classification Loss: 0.097379\n0 550 BBOX Regression Loss: 0.003137  Classification Loss: 0.095611\n0 560 BBOX Regression Loss: 0.003081  Classification Loss: 0.093905\n0 570 BBOX Regression Loss: 0.003027  Classification Loss: 0.092259\n0 580 BBOX Regression Loss: 0.002975  Classification Loss: 0.090670\n0 590 BBOX Regression Loss: 0.002925  Classification Loss: 0.089135\n0 600 BBOX Regression Loss: 0.002876  Classification Loss: 0.087650\n0 610 BBOX Regression Loss: 0.002829  Classification Loss: 0.086215\n0 620 BBOX Regression Loss: 0.002783  Classification Loss: 0.084825\n0 630 BBOX Regression Loss: 0.002739  Classification Loss: 0.083480\n0 640 BBOX Regression Loss: 0.002696  Classification Loss: 0.082177\n0 650 BBOX Regression Loss: 0.002655  Classification Loss: 0.080914\n0 660 BBOX Regression Loss: 0.002615  Classification Loss: 0.079688\n0 670 BBOX Regression Loss: 0.002576  Classification Loss: 0.078500\n0 680 BBOX Regression Loss: 0.002538  Classification Loss: 0.077346\n0 690 BBOX Regression Loss: 0.002501  Classification Loss: 0.076226\n0 700 BBOX Regression Loss: 0.002466  Classification Loss: 0.075138\n0 710 BBOX Regression Loss: 0.002431  Classification Loss: 0.074081\n0 720 BBOX Regression Loss: 0.002397  Classification Loss: 0.073052\n0 730 BBOX Regression Loss: 0.002364  Classification Loss: 0.072052\n0 740 BBOX Regression Loss: 0.002332  Classification Loss: 0.071079\n0 750 BBOX Regression Loss: 0.002301  Classification Loss: 0.070132\n0 760 BBOX Regression Loss: 0.002271  Classification Loss: 0.069210\n0 770 BBOX Regression Loss: 0.002242  Classification Loss: 0.068312\n0 780 BBOX Regression Loss: 0.002213  Classification Loss: 0.067436\n0 790 BBOX Regression Loss: 0.002185  Classification Loss: 0.066583\n0 800 BBOX Regression Loss: 0.002158  Classification Loss: 0.065752\n0 810 BBOX Regression Loss: 0.002131  Classification Loss: 0.064940\n0 820 BBOX Regression Loss: 0.002105  Classification Loss: 0.064149\n0 830 BBOX Regression Loss: 0.002080  Classification Loss: 0.063376\n0 840 BBOX Regression Loss: 0.002055  Classification Loss: 0.062622\n0 850 BBOX Regression Loss: 0.002031  Classification Loss: 0.061886\n0 860 BBOX Regression Loss: 0.002007  Classification Loss: 0.061167\n0 870 BBOX Regression Loss: 0.001984  Classification Loss: 0.060464\n0 880 BBOX Regression Loss: 0.001962  Classification Loss: 0.059777\n0 890 BBOX Regression Loss: 0.001940  Classification Loss: 0.059106\n0 900 BBOX Regression Loss: 0.001918  Classification Loss: 0.058450\n0 910 BBOX Regression Loss: 0.001897  Classification Loss: 0.057808\n0 920 BBOX Regression Loss: 0.001877  Classification Loss: 0.057180\n0 930 BBOX Regression Loss: 0.001857  Classification Loss: 0.056565\n0 940 BBOX Regression Loss: 0.001837  Classification Loss: 0.055964\n0 950 BBOX Regression Loss: 0.001817  Classification Loss: 0.055375\n0 960 BBOX Regression Loss: 0.001799  Classification Loss: 0.054799\n0 970 BBOX Regression Loss: 0.001780  Classification Loss: 0.054234\n0 980 BBOX Regression Loss: 0.001762  Classification Loss: 0.053681\n0 990 BBOX Regression Loss: 0.001744  Classification Loss: 0.053139\n0 1000 BBOX Regression Loss: 0.001727  Classification Loss: 0.052608\n1 10 BBOX Regression Loss: 0.000002  Classification Loss: 0.000026\n1 20 BBOX Regression Loss: 0.000001  Classification Loss: 0.000026\n1 30 BBOX Regression Loss: 0.000001  Classification Loss: 0.000026\n1 40 BBOX Regression Loss: 0.000001  Classification Loss: 0.000025\n1 50 BBOX Regression Loss: 0.000005  Classification Loss: 0.000025\n1 60 BBOX Regression Loss: 0.000005  Classification Loss: 0.000025\n1 70 BBOX Regression Loss: 0.000005  Classification Loss: 0.000025\n1 80 BBOX Regression Loss: 0.000004  Classification Loss: 0.000025\n1 90 BBOX Regression Loss: 0.000004  Classification Loss: 0.000024\n1 100 BBOX Regression Loss: 0.000004  Classification Loss: 0.000024\n1 110 BBOX Regression Loss: 0.000004  Classification Loss: 0.000024\n1 120 BBOX Regression Loss: 0.000005  Classification Loss: 0.000024\n1 130 BBOX Regression Loss: 0.000005  Classification Loss: 0.000024\n1 140 BBOX Regression Loss: 0.000005  Classification Loss: 0.000024\n1 150 BBOX Regression Loss: 0.000005  Classification Loss: 0.000023\n1 160 BBOX Regression Loss: 0.000005  Classification Loss: 0.000023\n1 170 BBOX Regression Loss: 0.000005  Classification Loss: 0.000023\n1 180 BBOX Regression Loss: 0.000005  Classification Loss: 0.000023\n1 190 BBOX Regression Loss: 0.000006  Classification Loss: 0.000023\n1 200 BBOX Regression Loss: 0.000006  Classification Loss: 0.000023\n1 210 BBOX Regression Loss: 0.000006  Classification Loss: 0.000022\n1 220 BBOX Regression Loss: 0.000005  Classification Loss: 0.000022\n1 230 BBOX Regression Loss: 0.000005  Classification Loss: 0.000022\n1 240 BBOX Regression Loss: 0.000005  Classification Loss: 0.000022\n1 250 BBOX Regression Loss: 0.000006  Classification Loss: 0.000022\n1 260 BBOX Regression Loss: 0.000008  Classification Loss: 0.000022\n1 270 BBOX Regression Loss: 0.000008  Classification Loss: 0.000022\n1 280 BBOX Regression Loss: 0.000008  Classification Loss: 0.000022\n1 290 BBOX Regression Loss: 0.000008  Classification Loss: 0.000021\n1 300 BBOX Regression Loss: 0.000008  Classification Loss: 0.000021\n1 310 BBOX Regression Loss: 0.000008  Classification Loss: 0.000021\n1 320 BBOX Regression Loss: 0.000007  Classification Loss: 0.000021\n1 330 BBOX Regression Loss: 0.000007  Classification Loss: 0.000021\n1 340 BBOX Regression Loss: 0.000007  Classification Loss: 0.000021\n1 350 BBOX Regression Loss: 0.000009  Classification Loss: 0.000021\n1 360 BBOX Regression Loss: 0.000009  Classification Loss: 0.000021\n1 370 BBOX Regression Loss: 0.000009  Classification Loss: 0.000021\n1 380 BBOX Regression Loss: 0.000009  Classification Loss: 0.000020\n1 390 BBOX Regression Loss: 0.000009  Classification Loss: 0.000020\n1 400 BBOX Regression Loss: 0.000009  Classification Loss: 0.000020\n1 410 BBOX Regression Loss: 0.000009  Classification Loss: 0.000020\n1 420 BBOX Regression Loss: 0.000008  Classification Loss: 0.000020\n1 430 BBOX Regression Loss: 0.000008  Classification Loss: 0.000020\n1 440 BBOX Regression Loss: 0.000008  Classification Loss: 0.000020\n1 450 BBOX Regression Loss: 0.000008  Classification Loss: 0.000020\n1 460 BBOX Regression Loss: 0.000009  Classification Loss: 0.000020\n1 470 BBOX Regression Loss: 0.000010  Classification Loss: 0.000020\n1 480 BBOX Regression Loss: 0.000011  Classification Loss: 0.000019\n1 490 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 500 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 510 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 520 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 530 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 540 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 550 BBOX Regression Loss: 0.000010  Classification Loss: 0.000019\n1 560 BBOX Regression Loss: 0.000012  Classification Loss: 0.000019\n1 570 BBOX Regression Loss: 0.000012  Classification Loss: 0.000019\n1 580 BBOX Regression Loss: 0.000012  Classification Loss: 0.000019\n1 590 BBOX Regression Loss: 0.000012  Classification Loss: 0.000019\n1 600 BBOX Regression Loss: 0.000012  Classification Loss: 0.000018\n1 610 BBOX Regression Loss: 0.000012  Classification Loss: 0.000018\n1 620 BBOX Regression Loss: 0.000012  Classification Loss: 0.000018\n1 630 BBOX Regression Loss: 0.000011  Classification Loss: 0.000018\n1 640 BBOX Regression Loss: 0.000013  Classification Loss: 0.000018\n1 650 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 660 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 670 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 680 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 690 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 700 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 710 BBOX Regression Loss: 0.000014  Classification Loss: 0.000018\n1 720 BBOX Regression Loss: 0.000013  Classification Loss: 0.000018\n1 730 BBOX Regression Loss: 0.000013  Classification Loss: 0.000018\n1 740 BBOX Regression Loss: 0.000013  Classification Loss: 0.000018\n1 750 BBOX Regression Loss: 0.000013  Classification Loss: 0.000017\n1 760 BBOX Regression Loss: 0.000016  Classification Loss: 0.000017\n1 770 BBOX Regression Loss: 0.000018  Classification Loss: 0.000017\n1 780 BBOX Regression Loss: 0.000018  Classification Loss: 0.000017\n1 790 BBOX Regression Loss: 0.000018  Classification Loss: 0.000017\n1 800 BBOX Regression Loss: 0.000018  Classification Loss: 0.000017\n1 810 BBOX Regression Loss: 0.000018  Classification Loss: 0.000017\n1 820 BBOX Regression Loss: 0.000017  Classification Loss: 0.000017\n1 830 BBOX Regression Loss: 0.000017  Classification Loss: 0.000017\n1 840 BBOX Regression Loss: 0.000017  Classification Loss: 0.000017\n1 850 BBOX Regression Loss: 0.000017  Classification Loss: 0.000017\n1 860 BBOX Regression Loss: 0.000017  Classification Loss: 0.000017\n1 870 BBOX Regression Loss: 0.000016  Classification Loss: 0.000017\n1 880 BBOX Regression Loss: 0.000016  Classification Loss: 0.000017\n1 890 BBOX Regression Loss: 0.000016  Classification Loss: 0.000017\n1 900 BBOX Regression Loss: 0.000019  Classification Loss: 0.000017\n1 910 BBOX Regression Loss: 0.000020  Classification Loss: 0.000017\n1 920 BBOX Regression Loss: 0.000020  Classification Loss: 0.000017\n1 930 BBOX Regression Loss: 0.000020  Classification Loss: 0.000017\n1 940 BBOX Regression Loss: 0.000020  Classification Loss: 0.000016\n1 950 BBOX Regression Loss: 0.000020  Classification Loss: 0.000016\n1 960 BBOX Regression Loss: 0.000019  Classification Loss: 0.000016\n1 970 BBOX Regression Loss: 0.000019  Classification Loss: 0.000016\n1 980 BBOX Regression Loss: 0.000019  Classification Loss: 0.000016\n1 990 BBOX Regression Loss: 0.000019  Classification Loss: 0.000016\n1 1000 BBOX Regression Loss: 0.000019  Classification Loss: 0.000016\n2 10 BBOX Regression Loss: 0.000016  Classification Loss: 0.000011\n2 20 BBOX Regression Loss: 0.000055  Classification Loss: 0.000011\n2 30 BBOX Regression Loss: 0.000082  Classification Loss: 0.000011\n2 40 BBOX Regression Loss: 0.000069  Classification Loss: 0.000012\n2 50 BBOX Regression Loss: 0.000058  Classification Loss: 0.000012\n2 60 BBOX Regression Loss: 0.000049  Classification Loss: 0.000012\n2 70 BBOX Regression Loss: 0.000043  Classification Loss: 0.000012\n2 80 BBOX Regression Loss: 0.000037  Classification Loss: 0.000011\n2 90 BBOX Regression Loss: 0.000033  Classification Loss: 0.000011\n2 100 BBOX Regression Loss: 0.000032  Classification Loss: 0.000011\n2 110 BBOX Regression Loss: 0.000047  Classification Loss: 0.000011\n2 120 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 130 BBOX Regression Loss: 0.000046  Classification Loss: 0.000011\n2 140 BBOX Regression Loss: 0.000043  Classification Loss: 0.000011\n2 150 BBOX Regression Loss: 0.000041  Classification Loss: 0.000011\n2 160 BBOX Regression Loss: 0.000038  Classification Loss: 0.000011\n2 170 BBOX Regression Loss: 0.000036  Classification Loss: 0.000011\n2 180 BBOX Regression Loss: 0.000034  Classification Loss: 0.000011\n2 190 BBOX Regression Loss: 0.000033  Classification Loss: 0.000011\n2 200 BBOX Regression Loss: 0.000036  Classification Loss: 0.000011\n2 210 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 220 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 230 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 240 BBOX Regression Loss: 0.000046  Classification Loss: 0.000011\n2 250 BBOX Regression Loss: 0.000045  Classification Loss: 0.000011\n2 260 BBOX Regression Loss: 0.000043  Classification Loss: 0.000011\n2 270 BBOX Regression Loss: 0.000042  Classification Loss: 0.000011\n2 280 BBOX Regression Loss: 0.000050  Classification Loss: 0.000011\n2 290 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 300 BBOX Regression Loss: 0.000050  Classification Loss: 0.000011\n2 310 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 320 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 330 BBOX Regression Loss: 0.000046  Classification Loss: 0.000011\n2 340 BBOX Regression Loss: 0.000045  Classification Loss: 0.000011\n2 350 BBOX Regression Loss: 0.000044  Classification Loss: 0.000011\n2 360 BBOX Regression Loss: 0.000042  Classification Loss: 0.000011\n2 370 BBOX Regression Loss: 0.000041  Classification Loss: 0.000011\n2 380 BBOX Regression Loss: 0.000040  Classification Loss: 0.000011\n2 390 BBOX Regression Loss: 0.000039  Classification Loss: 0.000011\n2 400 BBOX Regression Loss: 0.000039  Classification Loss: 0.000011\n2 410 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 420 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 430 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 440 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 450 BBOX Regression Loss: 0.000050  Classification Loss: 0.000011\n2 460 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 470 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 480 BBOX Regression Loss: 0.000047  Classification Loss: 0.000011\n2 490 BBOX Regression Loss: 0.000046  Classification Loss: 0.000011\n2 500 BBOX Regression Loss: 0.000045  Classification Loss: 0.000011\n2 510 BBOX Regression Loss: 0.000047  Classification Loss: 0.000011\n2 520 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 530 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 540 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 550 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 560 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 570 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 580 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 590 BBOX Regression Loss: 0.000053  Classification Loss: 0.000011\n2 600 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 610 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 620 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 630 BBOX Regression Loss: 0.000050  Classification Loss: 0.000011\n2 640 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 650 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 660 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 670 BBOX Regression Loss: 0.000053  Classification Loss: 0.000011\n2 680 BBOX Regression Loss: 0.000053  Classification Loss: 0.000011\n2 690 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 700 BBOX Regression Loss: 0.000052  Classification Loss: 0.000011\n2 710 BBOX Regression Loss: 0.000051  Classification Loss: 0.000011\n2 720 BBOX Regression Loss: 0.000050  Classification Loss: 0.000011\n2 730 BBOX Regression Loss: 0.000050  Classification Loss: 0.000011\n2 740 BBOX Regression Loss: 0.000049  Classification Loss: 0.000011\n2 750 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 760 BBOX Regression Loss: 0.000048  Classification Loss: 0.000011\n2 770 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 780 BBOX Regression Loss: 0.000056  Classification Loss: 0.000011\n2 790 BBOX Regression Loss: 0.000057  Classification Loss: 0.000011\n2 800 BBOX Regression Loss: 0.000056  Classification Loss: 0.000011\n2 810 BBOX Regression Loss: 0.000056  Classification Loss: 0.000011\n2 820 BBOX Regression Loss: 0.000055  Classification Loss: 0.000011\n2 830 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 840 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 850 BBOX Regression Loss: 0.000053  Classification Loss: 0.000011\n2 860 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 870 BBOX Regression Loss: 0.000058  Classification Loss: 0.000011\n2 880 BBOX Regression Loss: 0.000059  Classification Loss: 0.000011\n2 890 BBOX Regression Loss: 0.000058  Classification Loss: 0.000011\n2 900 BBOX Regression Loss: 0.000058  Classification Loss: 0.000011\n2 910 BBOX Regression Loss: 0.000057  Classification Loss: 0.000011\n2 920 BBOX Regression Loss: 0.000057  Classification Loss: 0.000011\n2 930 BBOX Regression Loss: 0.000056  Classification Loss: 0.000011\n2 940 BBOX Regression Loss: 0.000055  Classification Loss: 0.000011\n2 950 BBOX Regression Loss: 0.000055  Classification Loss: 0.000011\n2 960 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 970 BBOX Regression Loss: 0.000054  Classification Loss: 0.000011\n2 980 BBOX Regression Loss: 0.000053  Classification Loss: 0.000011\n2 990 BBOX Regression Loss: 0.000059  Classification Loss: 0.000011\n2 1000 BBOX Regression Loss: 0.000060  Classification Loss: 0.000011\n"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    n=0\n",
    "    loss_sum=[]\n",
    "    loc_loss=[]\n",
    "    conf_loss=[]\n",
    "    for number__ in range(iteration) :\n",
    "        '''要用Variable包裝tensor才能送入模型'''\n",
    "        if Use_cuda:\n",
    "            image_ = Variable(image_in.cuda())\n",
    "            y = [Variable(torch.tensor(label_0).cuda(), volatile=True),Variable(torch.tensor(label_1).cuda(), \n",
    "                volatile=True),Variable(torch.tensor(label_2).cuda(), volatile=True),Variable(torch.tensor(label_3).cuda(), volatile=True)]      \n",
    "        else:\n",
    "            image_ = Variable(image_in)\n",
    "            y = [Variable(torch.tensor(label_0), volatile=True),Variable(torch.tensor(label_1), \n",
    "                volatile=True),Variable(torch.tensor(label_2), volatile=True),Variable(torch.tensor(label_3), volatile=True)]\n",
    "\n",
    "        '''Forward Pass'''\n",
    "        out = net(image_)\n",
    "        \n",
    "        '''Regression Loss and Classification Loss'''\n",
    "        loss_l,loss_c = criterion(out,y )\n",
    "\n",
    "        \n",
    "        loss = loss_l+ loss_c\n",
    "        '''Backward'''\n",
    "        loss.backward()\n",
    "\n",
    "        loc_loss.append(loss_l.data.cpu().numpy())\n",
    "        conf_loss.append(loss_c.data.cpu().numpy())\n",
    "        loss_sum.append(loss.data.cpu().numpy())\n",
    "        '''更新參數'''\n",
    "        optimizer.step()\n",
    "        '''清空Gradients'''\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        n+=1\n",
    "        if n%10==0:\n",
    "            print('{:d} {:d} BBOX Regression Loss: {:f}  Classification Loss: {:f}'.format(epoch,n,np.mean(loc_loss),np.mean(conf_loss)))\n",
    "            #print('{:d} {:d} '.format(epoch,n,np.mean(conf_loss)))\n",
    "    '''儲存權重'''\n",
    "    torch.save(ssd_net.state_dict(),'weights/Ｗeights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}